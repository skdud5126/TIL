{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2강 실습\n",
    "\n",
    "### 1) 웹 크롤링을 통한 데이터 수집\n",
    "\n",
    "## **🔍 Objectives**\n",
    "\n",
    "### **1. 실습 개요**\n",
    "- 🌐 **웹 크롤링 실습**   \n",
    "- 🔄 **CSV 데이터 처리 실습**  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. 실습 진행 목적 및 배경**\n",
    "- 본 실습에서는 **정적 페이지와 동적 페이지 웹 크롤링**을 실습하며, 다양한 라이브러리의 활용법을 배웁니다.  \n",
    "- **JSON, CSV, XML**과 같은 다양한 데이터 포맷의 구조를 이해하고 이를 변환하거나 저장하는 방법을 학습합니다.  \n",
    "- 학습자는 데이터를 수집, 처리, 저장하는 유연한 워크플로우를 체득하게 됩니다.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. 실습 수행으로 얻어갈 수 있는 역량**\n",
    "- 실시간 데이터를 **수집**하고 **분석**하는 기술 습득  \n",
    "- 데이터를 다룰 때 **포맷 변환 및 저장 능력** 향상  \n",
    "- **Selenium**, **BeautifulSoup** 등 웹 크롤링 라이브러리 활용 능력 향상  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 실습 핵심 내용**\n",
    "- 💻 **동적 웹 크롤링 실습** (Selenium 활용)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prerequisites**\n",
    "\n",
    "```\n",
    "selenium>=4.0.0\n",
    "webdriver-manager>=3.8.5\n",
    "beautifulsoup4>=4.9.3\n",
    "requests>=2.25.1\n",
    "lxml>=4.6.2\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 크롤링 실습\n",
    "\n",
    "- HTML 구조 실습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc ='''<html><head><title>웹크롤링 연습</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b><h1>HTML 연습</h1></b></p>\n",
    "<p class=\"link\">인터넷 링크를 작성하는 방법은\n",
    "<a class=\"naver\" href='https://sports.news.naver.com/wfootball/index' id=\"link1\">네이버 스포츠 뉴스</a>와\n",
    "<a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">다음 스포츠 뉴스</a>처럼\n",
    "HTML 링크로 작성할 수 있습니다.\n",
    "</p>\n",
    "<p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
    "<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.\n",
    "</p>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "with open('웹크롤링 연습.html','w',encoding='utf-8') as f:\n",
    "  f.write(html_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 코드 작성 이후 실행하고 코랩 탐색기에서 '웹크롤링 연습.html'을 다운받아서 실행해보기\n",
    "- 각 링크를 보며 어떤 구조로 연결이 되었는지 생각해보기\n",
    "- HTML 태그에 따른 구조를 생각하며 보기\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BeautifulSoup\n",
    "\n",
    "### 1-1. bs4 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "     -------------------------------------- 147.9/147.9 KB 2.2 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\skdud\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# bs4를 import로 사용하기 위해 설치\n",
    "\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HTML 구조 실습에서 작성해서 저장한 html 문서를 다시 불러들여 문서를 파싱하여 사람이 이해하기 쉬운 언어로 변환하고 필요한 정보를 추출하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 문서 파싱(parsing)\n",
    "\n",
    "- 앞서 저장한 html 문서를 다시 읽어들여 html 태그의 형태를 그대로 유지하면서 파싱하는 방법인 BeautifulSoup 클래스를 사용하여 읽는다.\n",
    "\n",
    "- 이를 사람이 식벽ㄹ하기 좋게 태그 단위로 출력하는 prettify 메소드를 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><title>웹크롤링 연습</title></head>\n",
      "<body>\n",
      "<p class=\"title\"><b><h1>HTML 연습</h1></b></p>\n",
      "<p class=\"link\">인터넷 링크를 작성하는 방법은\n",
      "<a class=\"naver\" href=\"https://sports.news.naver.com/wfootball/index\" id=\"link1\">네이버 스포츠 뉴스</a>와\n",
      "<a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">다음 스포츠 뉴스</a>처럼\n",
      "HTML 링크로 작성할 수 있습니다.\n",
      "</p>\n",
      "<p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
      "<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.\n",
      "</p>\n",
      "</body></html>\n",
      "\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   웹크롤링 연습\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    <h1>\n",
      "     HTML 연습\n",
      "    </h1>\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"link\">\n",
      "   인터넷 링크를 작성하는 방법은\n",
      "   <a class=\"naver\" href=\"https://sports.news.naver.com/wfootball/index\" id=\"link1\">\n",
      "    네이버 스포츠 뉴스\n",
      "   </a>\n",
      "   와\n",
      "   <a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">\n",
      "    다음 스포츠 뉴스\n",
      "   </a>\n",
      "   처럼\n",
      "HTML 링크로 작성할 수 있습니다.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
      "   <h2>\n",
      "    크게\n",
      "   </h2>\n",
      "   작성할 수도 있고,\n",
      "   <h4>\n",
      "    작게\n",
      "   </h4>\n",
      "   작성할 수도 있습니다.\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bs4 모듈에서 BeautifulSoup 클래스를 import 하고 bs라는 약칭으로 사용\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# 이전에 생성한 HTML 파일('웹크롤링 연습.html)을 읽기 모드('r')로 열기\n",
    "with open('웹크롤링 연습.html', 'r',encoding='utf-8') as f:\n",
    "    # HTML파일의 내용을 읽어와서 변수 html_doc에 저장\n",
    "    html_doc = f.read()\n",
    "\n",
    "# BeautifuleSoup 객체 생성 - html_doc을 'html.parser'를 사용하여 파싱\n",
    "soup = bs(html_doc,'html.parser')\n",
    "\n",
    "# BeautifulSoup 객체 출력(HTML 구조를 그대로 출력)\n",
    "print(soup)\n",
    "\n",
    "# prettify() 메서드를 사용해 HTML 문서를 들여쓰기 및 정리하여 출력\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prettify()를 사용해 출력시키면 태그의 시작과 종료 위치, 상위 및 하위 태그의 종속 관계를 이해하기 쉬워진다.\n",
    "\n",
    "- 여기서 저장한 soup 객체는 html 태그가 그대로 살아있어서 이 태그들의 특성을 이용해 필요한 정보를 추출할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML 연습\n",
      "인터넷 링크를 작성하는 방법은\n",
      "네이버 스포츠 뉴스와\n",
      "다음 스포츠 뉴스처럼\n",
      "HTML 링크로 작성할 수 있습니다.\n",
      "\n",
      "그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
      "크게작성할 수도 있고, 작게 작성할 수도 있습니다.\n",
      "\n",
      "HTML 연습\n",
      "인터넷 링크를 작성하는 방법은\n",
      "네이버 스포츠 뉴스와\n",
      "다음 스포츠 뉴스처럼\n",
      "HTML 링크로 작성할 수 있습니다.\n",
      "\n",
      "그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
      "크게작성할 수도 있고, 작게 작성할 수도 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# soup 객체에서 모든 <p> 태그를 찾은 후 반복문을 통해 텍스트만 출력\n",
    "for p in soup.find_all('p'):\n",
    "    # 찾은 <p> 태그의 텍스트 내용을 출력\n",
    "    print(p.text)\n",
    "\n",
    "# soup 객체에서 CSS 선택자 'p'를 사용하여 <p> 태그를 찾은 후 텍스트 출력\n",
    "for p in soup.select('p'):\n",
    "    # 찾은 <p> 태그의 텍스트 내용을 출력\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과를 보면 soup 객체의 메소드인 title과 태그명들을 활용하여 필요한 부분만 정보를 추출할 수 있다.\n",
    "\n",
    "- find_all, select 함수를 이용하는 방법이 필요한 부분의 정보를 추출하는데 더 효율적이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. find 함수 활용\n",
    "\n",
    "- **find**함수는 태그, 속성과 속성값을 활용해서 HTML 문서의 텍스트를 추출할 수 있다.\n",
    "\n",
    "- **find_all**함수는 여러 개의 내용을 리스트 형태로 가져올 수 있으며, 특정 하나의 태그만을 추출하려면 find 함수를 사용한다.\n",
    "\n",
    "- 태그 내의 **class**혹은 **id** 등에 해당하는 속성과 속성값을 활용하여 해당 태그의 내용을 추출할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 <p>태그 : [<p class=\"title\"><b><h1>HTML 연습</h1></b></p>, <p class=\"link\">인터넷 링크를 작성하는 방법은\n",
      "<a class=\"naver\" href=\"https://sports.news.naver.com/wfootball/index\" id=\"link1\">네이버 스포츠 뉴스</a>와\n",
      "<a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">다음 스포츠 뉴스</a>처럼\n",
      "HTML 링크로 작성할 수 있습니다.\n",
      "</p>, <p class=\"story\">그리고 HTML 문서는 글자의 크기도 조정 가능해서\n",
      "<h2>크게</h2>작성할 수도 있고, <h4>작게</h4> 작성할 수도 있습니다.\n",
      "</p>]\n",
      "첫번째 <p>태그 :  <p class=\"title\"><b><h1>HTML 연습</h1></b></p>\n",
      "None\n",
      "<a class=\"naver\" href=\"https://sports.news.naver.com/wfootball/index\" id=\"link1\">네이버 스포츠 뉴스</a>\n",
      "<a class=\"daum\" href=\"https://sports.daum.net/worldsoccer\" id=\"link2\">다음 스포츠 뉴스</a>\n"
     ]
    }
   ],
   "source": [
    "# soup 객체에서 모든 <p>태그를 찾아 리스트 형태로 출력\n",
    "print(\"모든 <p>태그 :\", soup.find_all('p'))\n",
    "\n",
    "# soup 객체에서  첫 번째 <p> 태그를 찾아 출력\n",
    "print(\"첫번째 <p>태그 : \", soup.find('p'))\n",
    "\n",
    "# soup 객체에서 속성이 'class'이고 값이 'python'인 요소를 찾아 출력\n",
    "print(soup.find(attrs={'class':'python'}))\n",
    "\n",
    "# soup 객체에서 태그가 'a'이고, 속성 'id'가 'link1'인 요소를 찾아 출력\n",
    "print(soup.find('a',{'id':'link1'}))\n",
    "\n",
    "# soup 객체에서 속성이 'id'이고 값이 'link2'인 요소를 찾아 출력\n",
    "print(soup.find(attrs={'id':'link2'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본적으로 태그 명을 사용하고, 여기에 클래스나 아이디 등의 속성과 그에 해당하는 속성 값을 넣어 해당 태그의 텍스트를 추출할 수 있다.\n",
    "\n",
    "- 실제 값을 추출할 경우, find_all은 여러 개의 결과를 반환하므로, 각 결과를 처리하기 위해 for문을 사용한다.\n",
    "\n",
    "- find의 경우는 단일값을 가지고 있기 때문에 속성값을 추출할 때는 get 함수를 이용하고, 태그에 둘러싸여 있는 텍스트를 추출하고자 할 때는 text 메소드를 활용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML 연습\n",
      "['title']\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 <p> 태그 내부에서 <h1>태그를 찾아 텍스트 출력\n",
    "print(soup.find('p').find('h1').text)\n",
    "\n",
    "# 첫 번째 <p> 태그의 'class' 속성 값을 가져와 출력\n",
    "print(soup.find_all('p')[0].get('class'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
